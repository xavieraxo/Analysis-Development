version: '3.8'

services:
  gateway-api:
    build:
      context: ..
      dockerfile: src/Gateway.Api/Dockerfile
    ports:
      - "8094:8080"
      - "8081:8081"
    environment:
      - ASPNETCORE_URLS=http://+:8080
      - OpenAI__BaseUrl=http://ollama:11434/v1
      - OpenAI__Model=llama3.2
      - OpenAI__Key=
    depends_on:
      - ollama
    networks:
      - multiagent-network

  gateway-blazor:
    build:
      context: ..
      dockerfile: src/Gateway.Blazor/Dockerfile
    ports:
      - "8093:8080"
    environment:
      - ASPNETCORE_URLS=http://+:8080
      - ApiBaseUrl=http://gateway-api:8080
    depends_on:
      - gateway-api
    networks:
      - multiagent-network

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - multiagent-network
    # Para usar un modelo local, ejecutar despu√©s del inicio:
    # docker exec -it <container> ollama pull llama3.2

networks:
  multiagent-network:
    driver: bridge

volumes:
  ollama-data:

